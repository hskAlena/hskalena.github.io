<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hyunseo Kim</title>
  
  <meta name="author" content="Hyunseo Kim">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hyunseo Kim</name>
              </p>
              <p>I am a PhD student in <a href="https://brainscience.snu.ac.kr/">Interdisciplinary program in Neuroscience</a> at Seoul National University, 
		      and a member of the <a href="https://bi.snu.ac.kr/">biointelligence laboratory</a> led by Byoung-Tak Zhang. 
		</p>
              <p>
                 My research interests lie in the area of robotics, vision, imitation learning and representation learning.
              </p>
              <p style="text-align:center">
                <a href="mailto:hyunseo.kim159@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/20230623_hskimCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=ko&authuser=2&user=5R0JMRwAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hskalena/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/HyunseoKim.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/HyunseoKim.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, robotics, self-supervised learning, imitation learning, and representation learning. 
		      Much of my research is about inferring the object representation from related actions. 
		      Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
          <tr onmouseout="dream_stop()" onmouseover="dream_start()"  bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/abs1.png' width="160">
              </div>
              <script type="text/javascript">
                function dream_start() {
                  document.getElementById('dreamfusion_image').style.opacity = "0";
                }

                function dream_stop() {
                  document.getElementById('dreamfusion_image').style.opacity = "0";
                }
                dream_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.05262">
                <papertitle>EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object</papertitle>
              </a>
              <br>
              <strong>Hyunseo Kim</strong>,              
              Hye Jung Yoon,
              Minji Kim,
							<a href="https://dshan4585.github.io/">Dong-Sig Han</a>,
              Byoung-Tak Zhang
              <br>
              <em>ICRA</em>, 2023
              <br>              
              <a href="https://arxiv.org/abs/2306.05262">arXiv</a>
              <!-- /
              <a href="https://dreamfusion3d.github.io/gallery.html">off</a> -->
              <p></p>
              <p>
              EXOT is applied to the robot hand camera (wrist camera) and successfully detects the target object absence during object manipulation.
              EXOT is a single object tracker with an out-of-distribution classifier. 
              It makes safe robotic manipulation possible even when the target moves.
              </p>
            </td>
          </tr>

          <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='dreamfusion_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/mdirl_fig1.png' width="160">
              </div>
              <script type="text/javascript">
                function dreamfusion_start() {
                  document.getElementById('dreamfusion_image').style.opacity = "0";
                }

                function dreamfusion_stop() {
                  document.getElementById('dreamfusion_image').style.opacity = "0";
                }
                dreamfusion_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1f7b1ed763e9c75e4db74b49b76db5f-Paper-Conference.pdf">
                <papertitle>Robust Imitation via Mirror Descent Inverse Reinforcement Learning</papertitle>
              </a>
              <br>
              <a href="https://dshan4585.github.io/">Dong-Sig Han</a>,              
              <strong>Hyunseo Kim</strong>,
              Hyundo Lee,
							Je-Hwan Ryu,
              Byoung-Tak Zhang
              <br>
              <em>NeurIPS</em>, 2022
              <br>
              <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1f7b1ed763e9c75e4db74b49b76db5f-Paper-Conference.pdf">official paper</a> /              
              <a href="https://arxiv.org/abs/2210.11201">arXiv</a>
              <!-- /
              <a href="https://dreamfusion3d.github.io/gallery.html">off</a> -->
              <p></p>
              <p>
              MD-AIRL predicts a sequence of reward functions, which are iterative solutions for a constrained convex problem.
              </p>
            </td>
          </tr>
		  
          <tr onmouseout="samurai_stop()" onmouseover="samurai_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='samurai_image'>
                  <img src='images/mpart_fig1.png' width="160"></div>
                <img src='images/mpart_fig1.png' width="160">
              </div>
              <script type="text/javascript">
                function samurai_start() {
                  document.getElementById('samurai_image').style.opacity = "1";
                }

                function samurai_stop() {
                  document.getElementById('samurai_image').style.opacity = "0";
                }
                samurai_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://proceedings.mlr.press/v139/kim21e/kim21e.pdf">
                <papertitle>Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning</papertitle>
              </a>
              <br>
              Taehyeong Kim,
              Injune Hwang,
              Hyundo Lee,  
              <strong>Hyunseo Kim</strong>,
              Won-Seok Choi, 
              Joseph J Lim, 
              Byoung-Tak Zhang
              <br>
              <em>ICML</em>, 2021
              <br>
              <a href="http://proceedings.mlr.press/v139/kim21e/kim21e.pdf">official paper</a> /
              <a href="https://arxiv.org/abs/2012.01227">arXiv</a>
              <p></p>
              <p>
		MPART suggests successful active online learning that selects representative queries and proceeds efficient model update 
		      that does not forget important info as soon as a new data sample is observed.
              </p>
            </td>
          </tr>		


          <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <img src='images/lpart_fig1.PNG' width="160"></div>
                <img src='images/lpart_fig1.PNG' width="160">
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9054655">
                <papertitle>Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</papertitle>
              </a>
              <br>
              Taehyeong Kim, 
              Injune Hwang, 
              Gi-Cheon Kang, 
              Won-Seok Choi, 
              <strong>Hyunseo Kim</strong>, 
              Byoung-Tak Zhang
              <br>
              <em>ICASSP</em>, 2020
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9054655">Official paper</a>
              /
              <a href="https://arxiv.org/abs/2005.02137/">arXiv</a>
              <p></p>
              <p>
		LPART suggests semi-supervised online learning for real-world problems where labels are rarely given 
		      and the opportunity to access the same data is limited.              
              </p>
            </td>
          </tr>
					
          <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerf_supervision.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/nature_exfig9.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfsuper_start() {
                  document.getElementById('nerfsuper_image').style.opacity = "1";
                }

                function nerfsuper_stop() {
                  document.getElementById('nerfsuper_image').style.opacity = "0";
                }
                nerfsuper_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
							<a href="https://www.nature.com/articles/s41586-020-2167-2/">
                <papertitle>A neural circuit mechanism for mechanosensory feedback control of ingestion</papertitle>
              </a>
              <br>
              Dong-Yoon Kim, 
              Gyuryang Heo, 
              Minyoo Kim, 
              <strong>Hyunseo Kim</strong>, 
              Ju Ae Jin, 
              Hyun-Kyung Kim, 
              Sieun Jung, 
              Myungmo An, 
              Benjamin H Ahn, 
              Jong Hwi Park, 
              Han-Eol Park, 
              Myungsun Lee, 
              Jung Weon Lee, 
              Gary J Schwartz, 
              Sung-Yon Kim
              <br>
              <em>Nature</em>, 2020  
              <br>
							<a href="https://www.nature.com/articles/s41586-020-2167-2">official paper</a> 

              <p></p>
              <p>We revealed a neural circuit that relay mechanosensory feedback from the digestive tract to the brain. 
		      Neurons in parabrachial nucleus that express the prodynorphin gene monitor the intake of both fluids and solids.</p>
            </td>
          </tr>

        </tbody></table>

				
        <div class="row-left">
	<h3>Domestic Journal</h3>	

	Scene Graph Generation Framework using Image Region Description <br>
	<font size="2.5">Woo Suk Choi, <b>Yu-Jung Heo</b>, and Byoung-Tak Zhang<br>
	<i>KIISE Transactions on Computer Practices, Vol. 29, No. 12, Dec, 2023</i></font><br><br>	
		
	Efficient Compositional Translation Embedding for Visual Relationship Detection <br>
	<font size="2.5"><b>Yu-Jung Heo</b>, Eun-Sol Kim, Woo Suk Choi, Kyoung-Woon On and Byoung-Tak Zhang<br>
	<i>Journal of KIISE, Vol. 49, No. 7, Jul, 2022</i> <a href="https://doi.org/10.5626/JOK.2022.49.7.544">[pdf]</a></font><br><br>

	DramaQA: Character-Centered Video Story Understanding with Hierarchical QA <br>
	<font size="2.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Youwon Jang, Ahjeong Seo, Seungchan Lee, Minsu Lee and Byoung-Tak Zhang<br>
	<i>KIISE Transactions on Computer Practices, Vol. 27, No. 1, Jan, 2021</i> <a href="https://doi.org/10.5626/KTCP.2021.27.1.7">[pdf]</a></font><br><br>
		
	Analyzing and Solving GuessWhat?! <br>
	<font size='2.5'>Sang-Woo Lee, Cheolho Han, <b>Yu-Jung Heo</b>, Woo-Young Kang, Jae-Hyun Jun and Byoung-Tak Zhang<br>
	<i>Journal of KIISE, Vol. 45, No. 1, Jan, 2018</i> <a href="https://bi.snu.ac.kr/Publications/Journals/Domestic/KIISE45_1_SWLee.pdf">[pdf]</a></font><br><br>
	
	Robust Scheduling based on Daily Activity Learning by using Markov Decision Process and Inverse Reinforcement Learning <br>
	<font size='2.5'>Sang-Woo Lee, Dong-Hyun Kwak, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Woo-Young Kang, Ceyda Cinarel and Byoung-Tak Zhang<br>
	<i>KIISE Transactions on Computer Practices, Vol. 23, No. 10, Oct, 2017</i> <a href="https://bi.snu.ac.kr/Publications/Journals/Domestic/KIISE_TCP_23_10_SWLee.pdf">[pdf]</a></font><br><br>
	
	Regional Projection Histogram Matching and Linear Regression based Video Stabilization for a Moving Vehicle <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Min-Kook Choi, Hyun-Gyu Lee and Sang-Chul Lee<br>
	<i>Journal of Broadcast Engineering Vol. 19, No. 6, Nov, 2014</i> <a href="http://www.koreascience.or.kr/article/ArticleFullRecord.jsp?cn=BSGHC3_2014_v19n6_798">[pdf]</a></font><br><br>
	
	<h3>Domestic Conference</h3>
	Scene Graph Generation Model utilizing Image Region Descriptions <br>
	<font size="2.5">Woo Suk Choi, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang<br> 
	<i>Proc. Korea Computer Congress 2023 (KCC 2023)</i> </font><br>
	&#x2728 Best presentation award<br><br>
	
	Event Detection based on Predictive Uncertainty of User World Models <br>
	<font size="2.5"><b>Yu-Jung Heo</b>, Kibeom Kim, HoJoon Song, Hyejung Yoon and Byoung-Tak Zhang<br> 
	<i>Proc. Korea Computer Congress 2022 (KCC 2022)</i> </font><br>
	&#x2728 Award for 7 top-performing teams (announced at the ETRI human understanding AI challenge: Learning and Reasoning lifelog)<br><br>		
		
	Video Story Understanding with Multi-level Character Attention Model <br>
	<font size="2.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Ahjeong Seo, Youwon Jang, Minsu Lee and Byoung-Tak Zhang<br> 
	<i>Proc. Korea Computer Congress 2021 (KCC 2021)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2021/KCC2021_SHChoi.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
	
	Future State Generation for Action Prediction in Cross Domain <br>
	<font size="2.5">Hyunseo Kim, <b>Yu-Jung Heo</b>, Kibeom Kim and Byoung-Tak Zhang<br>
	<i>Proc. Korea Computer Congress 2021 (KCC 2021)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2021/KCC2021_HSKim.pdf">[pdf]</a></font><br><br>
		
	Knowledge-aware Visual Question Answering with Structural Attention Model <br>
	<font size="2.5"><b>Yu-Jung Heo</b>, Eun-Sol Kim, Woo Suk Choi and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2020 (KCC 2020)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2020/KCC2020_YJHeoKCZ.pdf">[pdf]</a></font><br><br>
	
	A study on Scene Graph Unification of Visual Semantic Knowledge using synonym <br>
	<font size="2.5">Woo Suk Choi, Kyoung-Woon on, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2020 (KCC 2020)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2020/KCC2020_WSChoiOHZ.pdf">[pdf]</a></font><br><br>
	
	A study on analysis of human and machine visual attention map for Visual Question Answering <br>
	<font size="2.5">Hyuk-Gi Lee, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2020 (KCC 2020)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2020/KCC2020_HGLeeHZ.pdf">[pdf]</a></font><br><br>
		
	DramaQA: Human Level Video Story Understanding through Multilevel Question-Answering <br>
	<font size="2.5">Seongho Choi, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Gi-Cheon Kang and Byoung-Tak Zhang <br>
	<i>Proc. Korea Software Congress 2019 (KSC 2019)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KSC2019/KSC2019_SHChoiOHKZ.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
		
	Compositional Structure Learning for Sequential Video Data <br>
	<font size="2.5">Kyoung-Woon On, Eun-Sol Kim, <b>Yu-Jung Heo</b>, and Byoung-Tak Zhang <br>
	<i>Proc. Korea Computer Congress 2019 (KCC 2019)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2019/KCC2019_kwon.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
		
	A Study on Object Detection Technology for an Improved Visual Relationship Detection <br>
	<font size="2.5">Hyunji Choi, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2019 (KCC 2019)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2019/KCC2019_hyunjiChoi.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
		
	Analysis of Learning Strategy in AQM Framework for Goal-Oriented Visual Dialogue <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Sang-Woo Lee and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2018 (KCC 2018)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2018/KCC2018_yjheo.pdf">[pdf]</a></font><br><br>
	
	Comparison of Generative Classification Model and Discriminative Classification Model for AQM Framework <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Sang-Woo Lee and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2017 (KSC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KSC2017/KSC2017_YuJungHeo.pdf">[pdf]</a></font><br><br>
	
	Structural Knowledge Representation Learning for Content-based Question Answering <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Kyoung-Woon On, Eun-Sol Kim and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2017 (KCC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2017S/11-253.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
	
	Analyzing and Solving GuessWhat?! <br>
	<font size='2.5'>Sang-Woo Lee, Cheolho Han, <b>Yu-Jung Heo</b>, Woo-Young Kang, Jae-Hyun Jun and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2017 (KCC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2017S/11-257.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
	
	Goal-oriented Question Generator model using Attention for GuessWhat?! <br>
	<font size='2.5'>Jae-Hyun Jun, Woo-Young Kang, Cheolho Han, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2017 (KCC 2017)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KCC2017S/11-288.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
	
	Adaptive Question Answering System for Personalized Language Education <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Eun-Sol Kim, Kyoung-Woon On and Byoung-Tak Zhang <br> 
	<i>Proc. Korean Institute of Intelligence Systems Spring Conference 2017 (KIIS 2017)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/KIIS2017_yjheo.pdf">[pdf]</a></font><br><br>
	
	Multimodal Story Learning with Dynamic Memory Construction <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Eun-Sol Kim, Kyoung-Woon On and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2016 (KSC 2016)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2016w_YJHeo.pdf">[pdf]</a></font><br><br>
			
	Robust Scheduling based on Daily Activity Learning by using Markov Decision Process and Inverse Reinforcement Learning <br>
	<font size='2.5'>Sang-Woo Lee, Dong-Hyun Kwak, Kyoung-Woon On, <b>Yu-Jung Heo</b>, Woo-Young Kang, Ceyda Cinarel and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2016 (KSC 2016)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2016w_SLee.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>
		
	Dual Deep Memories for Video Question Answering <br>
	<font size='2.5'>Kyung-Min Kim, Changjun Nan, Jung-Woo Ha, <b>Yu-Jung Heo</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2016 (KSC 2016)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2015W_KimKM.pdf">[pdf]</a></font><br>
	&#x2728 Best presentation award<br><br>

	Pororobot: A Deep Learning Robot that Plays Video Q&A Games <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Kyung-Min Kim, and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2015 (KSC 2015)</i> <a href="https://bi.snu.ac.kr/Publications/Conferences/Domestic/KIISE2015W_HeoYJ.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
		
	Automated Visualization Methodology for Surface of Driving Road by Extracting Motion Parameters of Road Images <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Bo-Gyu Park, Hyun-Gyu Lee, Min-Kook Choi and Sang-Chul Lee <br> 
	<i>Workshop on Image Processing and Image Understanding 2015 (IPIU 2015)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/IPIU2015_yjheo.pdf">[pdf]</a></font><br><br>		
	
	Classification of Driving Events using Multi-sensor and Visualization of Driving Information <br>
	<font size='2.5'>Bo-Gyu Park, <b>Yu-Jung Heo</b>, Hyun-Gyu Lee, Min-Kook Choi and Sang-Chul Lee <br> 
	<i>Workshop on Image Processing and Image Understanding 2015 (IPIU 2015)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/IPIU2015_bgpark.pdf">[pdf]</a></font><br><br>
			
	Regional Projection Histogram Matching and Linear Regression based Video Stabilization for a Moving Vehicle <br>
	<font size='2.5'><b>Yu-Jung Heo</b>, Min-Kook Choi, Hyun-Gyu Lee, and Sang-Chul Lee <br> 
	<i>Proc. Korean Institute of Broadcast and Media Engineers summer conference 2014 (KIBME 2014)</i> <a href="https://bi.snu.ac.kr/~yjheo/paper/KIBME2014_yjheo.pdf">[pdf]</a></font><br>
	&#x2728 Best paper award<br><br>
	
	<em>Last update: June 2025 by Yu-Jung Heo</em>
	
	</div>
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                The template of this website is from <a href="https://github.com/jonbarron/jonbarron_website">jon barron source code</a>
              
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
