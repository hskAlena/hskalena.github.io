<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge; chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> Hyunseo Kim </title>

<!-- Bootstrap -->
<link href="css/bootstrap.css" rel="stylesheet">
<link href="style.css" rel="stylesheet" type="text/css">
</style>
</head>

<body><center>
	<div id="wrap">
	<div id="emptytop">
	</div>
	<div id="header", style='line-height:150%'>
	
	<div class="container">	
	<div class="row">
	  <div class="col-md-4"><img class="thumbnail" src="images/HyunseoKim.jpg" style="width: 160px; height: 200px;">
	  </div>
	  <div class="col-md-8">
	  <font size='6'><b>Hyunseo Kim (ÍπÄÌòÑÏÑú)</b></font><!--<a href="https://yujungheo.github.io/">[CV]</a>--><br><br>
	  I am a PhD student in <a href="https://brainscience.snu.ac.kr/">Interdisciplinary program in Neuroscience</a> at Seoul National University, 
		      and a member of the <a href="https://bi.snu.ac.kr/">biointelligence laboratory</a> led by <a href="https://bi.snu.ac.kr/~btzhang">Byoung-Tak Zhang</a>. <br>
	  
	  <b>Reseach interest: </b> Robotics, Computer Vision, 3D Vision in Robotics <br>
	  <b>Email : </b> hskhexasu@snu.ac.kr | hyunseo.kim159@gmail.com<br>

                <a href="data/250926-hskim_resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=ko&authuser=2&user=5R0JMRwAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hskalena/">Github</a> &nbsp/&nbsp
				<a href="https://www.linkedin.com/in/hyunseo-kim-50a1b7160">LinkedIn</a>
              <br>
	  </div>
	</div>
	
	<!-- <div class="well">
	<b>[2025/06]</b> Our paper on evaluating multicultural VLMs is accepted at ACL 2025<br>
	<b>[2024/07]</b> Our paper on multimodal dialogue response generation is accepted at ECCV 2024<br>
	<b>[2024/06]</b> Our team ranked 1st place in <a href="https://smartdataset.github.io/smart101/">The Smart-101 Challenge</a> at <a href="https://marworkshop.github.io/cvpr24/index.html">Multimodal Algorithmic Reasoning Workshop</a>, CVPR 2024
	<span style="font-size:15px;">üèÜ</span><br>
	<b>[2024/05]</b> Our paper on translation artifacts in cross-lingual visual question answering is accepted at ACL 2024 Findings<br>
	<b>[2024/04]</b> Our paper on evaluation metrics for multimodal story understanding is accepted at COGSCI 2024<br>
	<b>[2024/02]</b> Our paper on structure-aware multimodal sequential learning is accepted at AAAI 2024<br>
	<b>[2023/10]</b> Our article on video turing test is published in AI Magazine<br>
	<b>[2023/06]</b> I joined AI2X Lab, Tech Innovation Group, KT <span style="font-size:15px;">üí°</span><br>
	<b>[2022/02]</b> Our paper on knowledge-based visual question answering is accepted at ACL 2022<br>
	
	<details>
	<summary> <b> ... Click to see more news! (until 2021) </b></summary>
        <b>[2021/10]</b> Our paper on video turing test is accepted at AAAI-FSS 2021<br>
	<b>[2021/02]</b> Our paper on video story understanding is accepted at AAAI 2021<br>
	<b>[2020/06]</b> Our team ranked 1st place in <a href="http://activity-net.org/challenges/2020/tasks/guest_anet_eol.html">The 1st ActivityNet Entities Object Localization challenge</a> at <a href="http://activity-net.org/challenges/2020/program.html">International Challenge on Activity Recognition (ActivityNet)</a>, CVPR 2020
	<span style="font-size:15px;">üèÜ</span><br>
	<b>[2020/03]</b> Our paper on multimodal learning is accepted at CVPR 2020<br>
	<b>[2020/02]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Workshop/2020">The 2nd workshop on Video Turing Test: Toward Human-Level Video Story Understanding</a> and <a href="https://dramaqa.snu.ac.kr/Challenge/2020">the 2nd DramaQA Challenge</a> in ECCV 2020<br>
	<b>[2019/12]</b> Our paper on compositional structure learning is accepted at AAAI 2020 (oral)<br>
	<b>[2019/12]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Challenge/2019">The 1st DramaQA Challenge</a> in KSC 2019<br>
	<b>[2019/10]</b> I'm co-organizing <a href="https://dramaqa.snu.ac.kr/Workshop/2019">The 1st workshop on Video Turing Test: Toward Human-Level Video Story Understanding</a> in ICCV 2019<br>
	<b>[2019/06]</b> Our team ranked 1st place in <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/225/leaderboard/733">The 1st GQA challenge</a> at <a href="https://visualqa.org/workshop_2019.html">Visual Question Answering and Dialog Workshop</a> in CVPR 2019
	<span style="font-size:15px;">üèÜ</span><br>
	<b>[2019/01-2019/05]</b> I interned at Kakao Brain, Pangyo, Seongnam, Korea <br>
	<b>[2018/12]</b> Our paper on goal-oriented visual dialogue is accepted at NeurIPS 2018 (spotlight) <br>
	<b>[2018/08]</b> Our team ranked 5/312~1.6% (in-the-money) in <a href="https://www.kaggle.com/c/youtube8m-2018/leaderboard">The 2nd YouTube-8M Video Understanding challenge, ECCV 2018</a>
	</details>
    </div> -->

	<div class="row">
	<div class="col-md-12" style="text-align: right"><font size="1.5">*Authors contributed equally</font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><img class="thumbnail" src="images/abs1.png" style="width: 120px; height: 70px;"></div>
		<div class="col-md-10"><font size="3">AffoRo-GS: Few-shot 3D Affordance Learning for Open-vocabulary Robotic Manipulation with Gaussian Splatting</font>
		<font size="1.5"><b>Hyunseo Kim</b>, <a href="https://yeonjisong.github.io/">Yeon-Ji Song</a>, <a href="https://scholar.google.com/citations?user=75_DkUwAAAAJ&hl=ko">Minsu Lee</a> and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>Under review at IEEE Robotics and Automation Letters</i></b> </font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/2405.02568" class=""><img class="thumbnail" src="images/abs1.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Active Neural 3D Reconstruction with Colorized Surface Voxel-based View Selection</font>
		<font size="1.5"><b>Hyunseo Kim</b>, <a href="https://bi.snu.ac.kr/members/hyeonseo-yang.html">Hyeonseo Yang</a>, <a href="https://scholar.google.com/citations?user=u-9bdkwAAAAJ&hl=en">Taekyung Kim</a>, 
			<a href="https://openreview.net/profile?id=~Yoonsung_Kim2">YoonSung Kim</a>, <a href="https://wityworks.com/">Jin-Hwa Kim</a> and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>Arkiv 2024</i></b> <a href="https://arxiv.org/abs/2405.02568"> [pdf]<a href="https://github.com/hskAlena/CSV">  [code]</a></a></font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><img class="thumbnail" src="images/abs1.png" style="width: 120px; height: 70px;"></div>
		<div class="col-md-10"><font size="3">Learning Object Motion and Appearance Dynamics with Object-Centric Representations</font>
		<font size="1.5"><a href="https://yeonjisong.github.io/">Yeon-Ji Song</a>, <b>Hyunseo Kim</b>, <a href="https://openreview.net/profile?id=~Suhyung_Choi2">Suhyung Choi</a>, <a href="https://wityworks.com/">Jin-Hwa Kim</a> and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>Causal Representation Learning Workshop at Neural Information Processing Systems (NeurIPS), 2023</i></b> </font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://arxiv.org/abs/2306.05262" class=""><img class="thumbnail" src="images/abs1.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object</font>
		<font size="1.5"><b>Hyunseo Kim</b>, <a href="https://scholar.google.com/citations?user=wtHogkAAAAAJ&hl=ko">Hye Jung Yoon</a>, <a href="https://bi.snu.ac.kr/members/minji-kim.html">Minji Kim</a>, <a href="https://dshan4585.github.io/">Dong-Sig Han</a> and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>ICRA 2023</i></b> (acceptance ratio: 1345/3125~43%) <a href="https://arxiv.org/abs/2306.05262"> [pdf]<a href="https://github.com/hskAlena/EXOT">  [code]</a></a></font></div>
	</div>
	
	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c1f7b1ed763e9c75e4db74b49b76db5f-Paper-Conference.pdf" class=""><img class="thumbnail" src="images/mdirl_fig1.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Robust Imitation via Mirror Descent Inverse Reinforcement Learning</font><br>
		<font size="1.5"><a href="https://dshan4585.github.io/">Dong-Sig Han</a>,  <b>Hyunseo Kim</b>, Hyundo Lee, <a href="https://openreview.net/profile?id=~JeHwan_Ryu2">Je-Hwan Ryu</a> and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>NeurIPS 2022 </i></b><i> (acceptance ratio: 2671/10411~25.66%) </i><a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/c1f7b1ed763e9c75e4db74b49b76db5f-Paper-Conference.pdf"> [pdf]<a href="https://github.com/dshan4585/mdirl">  [code]</a></a></font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="http://proceedings.mlr.press/v139/kim21e/kim21e.pdf" class=""><img class="thumbnail" src="images/mpart_fig1.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Message Passing Adaptive Resonance Theory for Online Active Semi-supervised Learning</font><br>
		<font size="1.5">Taehyeong Kim, Injune Hwang, <a href="https://scholar.google.com/citations?user=tqH-cckAAAAJ&hl=ko">Hyundo Lee</a>, <b>Hyunseo Kim</b>, Won-Seok Choi, Joseph J Lim and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>ICML 2021 </i></b><a href=http://proceedings.mlr.press/v139/kim21e/kim21e.pdf"> [pdf]</a></font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://ieeexplore.ieee.org/abstract/document/9054655" class=""><img class="thumbnail" src="images/lpart_fig1.PNG" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</font><br>
		<font size="1.5"><a href="https://scholar.google.com/citations?user=niiIBS0AAAAJ&hl=ko">Taehyeong Kim</a>, <a href="https://scholar.google.com/citations?user=haW9gXcAAAAJ&hl=ko">Injune Hwang</a>, 
			<a href="https://gicheonkang.com/">Gi-Cheon Kang</a>, <a href="https://scholar.google.com/citations?user=A178IhQAAAAJ&hl=ko">Won-Seok Choi</a>, <b>Hyunseo Kim</b> and Byoung-Tak Zhang</font><br>
		<font size="2.5"><b><i>ICASSP 2020 </i></b><a href=https://arxiv.org/abs/2005.02137/"> [pdf]</a></font></div>
	</div>

	<div class="row">
		<span style="line-height:10%"><br></span>
		<div class="col-md-2"><a href="https://www.nature.com/articles/s41586-020-2167-2/" class=""><img class="thumbnail" src="images/nature_exfig9.png" style="width: 120px; height: 70px;"></a></div>
		<div class="col-md-10"><font size="3">A neural circuit mechanism for mechanosensory feedback control of ingestion</font><br>
		<font size="1.5">Dong-Yoon Kim*, Gyuryang Heo*, Minyoo Kim*, <b>Hyunseo Kim</b>, Ju Ae Jin, Hyun-Kyung Kim, Sieun Jung, Myungmo An, Benjamin H Ahn, Jong Hwi Park, Han-Eol Park, 
              Myungsun Lee, Jung Weon Lee, Gary J Schwartz and Sung-Yon Kim</font><br>
		<font size="2.5"><b><i>Nature 2020 </i></b><a href=https://www.nature.com/articles/s41586-020-2167-2/"> [pdf]</a></font></div>
	</div>
				
    <div class="row-left">
	<h3>Domestic Conference</h3>
		
	Refined Object Tracking deducing Exit and Entrance signal with Large Multimodal Model <br>
	<font size='2.5'><b>Hyunseo Kim</b> and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2024 (KCC 2024)</i></font><br><br>	

	Bottleneck-Aware Linear Augmentation for Robotic Imitation Learning <br>
	<font size='2.5'>Minji Kim, Ganghun Lee, <b>Hyunseo Kim</b>, Minsu Lee and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2024 (KCC 2024)</i></font><br><br>	

	Self-Playing Reinforcement Learning Framework for Starcraft 2 Mini-Game <br>
	<font size='2.5'>Moonhoen Lee, <b>Hyunseo Kim</b>, Minji Kim, Juno Kim, Hye Jung Yoon and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2023 (KSC 2023)</i></font><br><br>
	
	Future State Generation for Action Prediction in Cross Domain <br>
	<font size='2.5'><b>Hyunseo Kim</b>, <a href="https://yujungheo.github.io/">Yu-Jung Heo</a>, Kibeom Kim and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Computer Congress 2021 (KCC 2021)</i></font><br><br>
			
	Predicting health indicator using Vector autoregression <br>
	<font size='2.5'><b>Hyunseo Kim</b>, Won Seok Choi and Byoung-Tak Zhang <br> 
	<i>Proc. Korea Software Congress 2019 (KSC 2019)</i></font><br><br>
	
	<em>Last update: September 2025 by Hyunseo Kim</em>
	
	</div>
	
	<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
	</div>
</center></body>
</html>
